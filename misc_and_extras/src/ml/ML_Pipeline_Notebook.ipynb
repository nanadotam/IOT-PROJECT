{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üêî Smart Poultry Heater Control System - ML Pipeline\n",
    "\n",
    "## Complete Machine Learning Pipeline for IoT Heater Control\n",
    "\n",
    "This notebook performs:\n",
    "1. **Data Exploration** - Analyze the dataset and visualize patterns\n",
    "2. **Model Training** - Train and compare multiple ML models\n",
    "3. **Hyperparameter Tuning** - Optimize the best model\n",
    "4. **Model Quantization** - Prepare for embedded deployment\n",
    "5. **Export Artifacts** - Generate deployment files\n",
    "\n",
    "---\n",
    "\n",
    "**Dataset:** 60,000 samples of Temperature, Humidity, LDR (Light) ‚Üí Heater ON/OFF\n",
    "\n",
    "**Goal:** Predict when to turn the heater ON or OFF based on environmental conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üì¶ Step 1: Install & Import Required Libraries\n",
    "\n",
    "First, let's install and import all the necessary libraries for our ML pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment if running in Colab)\n",
    "# !pip install pandas numpy matplotlib seaborn scikit-learn joblib\n",
    "\n",
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier, export_text, plot_tree\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report, roc_auc_score, roc_curve\n",
    ")\n",
    "import joblib\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìÇ Step 2: Upload & Load Dataset\n",
    "\n",
    "Upload your `data_for_IoT.csv` file to Colab, or specify the path if it's already available.\n",
    "\n",
    "**Dataset Structure:**\n",
    "- `Temp` - Temperature (¬∞C)\n",
    "- `Humidity` - Humidity (%)\n",
    "- `LDR` - Light intensity (0-100)\n",
    "- `Heater` - Target variable (0=OFF, 1=ON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload file in Colab (uncomment if needed)\n",
    "# from google.colab import files\n",
    "# uploaded = files.upload()\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('data_for_IoT.csv')\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üìä DATASET LOADED\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\n‚úì Shape: {df.shape}\")\n",
    "print(f\"  Rows: {df.shape[0]:,}\")\n",
    "print(f\"  Columns: {df.shape[1]}\")\n",
    "\n",
    "print(\"\\nüìã First 10 rows:\")\n",
    "display(df.head(10))\n",
    "\n",
    "print(\"\\nüìä Dataset Info:\")\n",
    "df.info()\n",
    "\n",
    "print(\"\\nüìà Statistical Summary:\")\n",
    "display(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîç Step 3: Data Exploration & Analysis\n",
    "\n",
    "Let's explore the dataset to understand:\n",
    "- Missing values\n",
    "- Class distribution\n",
    "- Feature ranges\n",
    "- Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"üîç DATA EXPLORATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nüîé Missing Values:\")\n",
    "missing = df.isnull().sum()\n",
    "print(missing)\n",
    "if missing.sum() == 0:\n",
    "    print(\"‚úì No missing values found!\")\n",
    "\n",
    "# Check for duplicates\n",
    "duplicates = df.duplicated().sum()\n",
    "print(f\"\\nüîé Duplicate Rows: {duplicates:,}\")\n",
    "\n",
    "# Class distribution\n",
    "print(\"\\nüéØ Target Variable Distribution (Heater):\")\n",
    "heater_dist = df['Heater'].value_counts()\n",
    "print(heater_dist)\n",
    "print(f\"\\nClass Balance:\")\n",
    "print(f\"  OFF (0): {heater_dist[0]:,} ({heater_dist[0]/len(df)*100:.2f}%)\")\n",
    "print(f\"  ON  (1): {heater_dist[1]:,} ({heater_dist[1]/len(df)*100:.2f}%)\")\n",
    "\n",
    "# Feature ranges\n",
    "print(\"\\nüìè Feature Ranges:\")\n",
    "for col in ['Temp', 'Humidity', 'LDR']:\n",
    "    print(f\"  {col:10s}: [{df[col].min():.1f}, {df[col].max():.1f}]\")\n",
    "\n",
    "# Correlation analysis\n",
    "print(\"\\nüîó Correlation Matrix:\")\n",
    "corr_matrix = df.corr()\n",
    "display(corr_matrix)\n",
    "\n",
    "print(\"\\nüéØ Correlation with Heater (sorted):\")\n",
    "heater_corr = corr_matrix['Heater'].sort_values(ascending=False)\n",
    "print(heater_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìä Step 4: Data Visualizations\n",
    "\n",
    "Create comprehensive visualizations to understand the data patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Feature Distributions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Feature Distributions and Target Variable', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Temperature\n",
    "axes[0, 0].hist(df['Temp'], bins=30, color='#FF6B6B', alpha=0.7, edgecolor='black')\n",
    "axes[0, 0].set_title('Temperature Distribution', fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Temperature (¬∞C)')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Humidity\n",
    "axes[0, 1].hist(df['Humidity'], bins=30, color='#4ECDC4', alpha=0.7, edgecolor='black')\n",
    "axes[0, 1].set_title('Humidity Distribution', fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Humidity (%)')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# LDR\n",
    "axes[1, 0].hist(df['LDR'], bins=30, color='#FFE66D', alpha=0.7, edgecolor='black')\n",
    "axes[1, 0].set_title('Light Intensity (LDR) Distribution', fontweight='bold')\n",
    "axes[1, 0].set_xlabel('LDR Value (0-100)')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Heater\n",
    "heater_counts = df['Heater'].value_counts()\n",
    "colors = ['#95E1D3', '#F38181']\n",
    "axes[1, 1].bar(['OFF (0)', 'ON (1)'], heater_counts.values, color=colors, alpha=0.7, edgecolor='black')\n",
    "axes[1, 1].set_title('Heater State Distribution', fontweight='bold')\n",
    "axes[1, 1].set_ylabel('Count')\n",
    "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Feature distributions plotted!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Correlation Heatmap\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "sns.heatmap(df.corr(), annot=True, cmap='coolwarm', center=0, \n",
    "            square=True, linewidths=1, cbar_kws={\"shrink\": 0.8}, ax=ax)\n",
    "ax.set_title('Feature Correlation Heatmap', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Correlation heatmap plotted!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Box Plots by Heater State\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "fig.suptitle('Feature Distributions by Heater State', fontsize=16, fontweight='bold')\n",
    "\n",
    "features = ['Temp', 'Humidity', 'LDR']\n",
    "\n",
    "for idx, feature in enumerate(features):\n",
    "    sns.boxplot(data=df, x='Heater', y=feature, palette=['#95E1D3', '#F38181'], ax=axes[idx])\n",
    "    axes[idx].set_title(f'{feature} by Heater State', fontweight='bold')\n",
    "    axes[idx].set_xlabel('Heater State')\n",
    "    axes[idx].set_xticklabels(['OFF (0)', 'ON (1)'])\n",
    "    axes[idx].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Box plots created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîß Step 5: Data Preparation\n",
    "\n",
    "Split the data into training and testing sets.\n",
    "\n",
    "- **Training Set:** 80% (48,000 samples)\n",
    "- **Test Set:** 20% (12,000 samples)\n",
    "- **Stratified Split:** Maintains class balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"üîß PREPARING DATA\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Separate features and target\n",
    "X = df[['Temp', 'Humidity', 'LDR']]\n",
    "y = df['Heater']\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úì Data split completed:\")\n",
    "print(f\"  Training set: {len(X_train):,} samples ({len(X_train)/len(X)*100:.1f}%)\")\n",
    "print(f\"  Test set:     {len(X_test):,} samples ({len(X_test)/len(X)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\n‚úì Class distribution in training set:\")\n",
    "train_dist = y_train.value_counts()\n",
    "print(f\"  OFF (0): {train_dist[0]:,} ({train_dist[0]/len(y_train)*100:.2f}%)\")\n",
    "print(f\"  ON  (1): {train_dist[1]:,} ({train_dist[1]/len(y_train)*100:.2f}%)\")\n",
    "\n",
    "print(\"\\n‚úì Data preparation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ü§ñ Step 6: Train Multiple Models\n",
    "\n",
    "We'll train and compare 4 different models:\n",
    "1. **Logistic Regression** (baseline)\n",
    "2. **Decision Tree**\n",
    "3. **Random Forest**\n",
    "4. **Gradient Boosting**\n",
    "\n",
    "Each model will be evaluated on:\n",
    "- Accuracy\n",
    "- Precision\n",
    "- Recall\n",
    "- F1 Score\n",
    "- ROC AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"ü§ñ TRAINING MODELS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42, max_depth=10),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42, n_estimators=100, max_depth=10),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(random_state=42, n_estimators=100, max_depth=5)\n",
    "}\n",
    "\n",
    "# Store results\n",
    "results = {}\n",
    "\n",
    "print(\"\\nTraining models... (this may take a few minutes)\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Logistic Regression\n",
    "print(\"‚îÄ\" * 80)\n",
    "print(\"üîÑ Training: Logistic Regression\")\n",
    "print(\"‚îÄ\" * 80)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "lr_model = models['Logistic Regression']\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "y_pred = lr_model.predict(X_test_scaled)\n",
    "y_pred_proba = lr_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Calculate metrics\n",
    "results['Logistic Regression'] = {\n",
    "    'model': lr_model,\n",
    "    'accuracy': accuracy_score(y_test, y_pred),\n",
    "    'precision': precision_score(y_test, y_pred),\n",
    "    'recall': recall_score(y_test, y_pred),\n",
    "    'f1': f1_score(y_test, y_pred),\n",
    "    'roc_auc': roc_auc_score(y_test, y_pred_proba),\n",
    "    'y_pred': y_pred,\n",
    "    'y_pred_proba': y_pred_proba,\n",
    "    'confusion_matrix': confusion_matrix(y_test, y_pred)\n",
    "}\n",
    "\n",
    "print(f\"\\nüìä Results:\")\n",
    "print(f\"  Accuracy:  {results['Logistic Regression']['accuracy']:.4f}\")\n",
    "print(f\"  Precision: {results['Logistic Regression']['precision']:.4f}\")\n",
    "print(f\"  Recall:    {results['Logistic Regression']['recall']:.4f}\")\n",
    "print(f\"  F1 Score:  {results['Logistic Regression']['f1']:.4f}\")\n",
    "print(f\"  ROC AUC:   {results['Logistic Regression']['roc_auc']:.4f}\")\n",
    "print(\"\\n‚úì Logistic Regression trained!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Decision Tree\n",
    "print(\"‚îÄ\" * 80)\n",
    "print(\"üîÑ Training: Decision Tree\")\n",
    "print(\"‚îÄ\" * 80)\n",
    "\n",
    "dt_model = models['Decision Tree']\n",
    "dt_model.fit(X_train, y_train)\n",
    "y_pred = dt_model.predict(X_test)\n",
    "y_pred_proba = dt_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "results['Decision Tree'] = {\n",
    "    'model': dt_model,\n",
    "    'accuracy': accuracy_score(y_test, y_pred),\n",
    "    'precision': precision_score(y_test, y_pred),\n",
    "    'recall': recall_score(y_test, y_pred),\n",
    "    'f1': f1_score(y_test, y_pred),\n",
    "    'roc_auc': roc_auc_score(y_test, y_pred_proba),\n",
    "    'y_pred': y_pred,\n",
    "    'y_pred_proba': y_pred_proba,\n",
    "    'confusion_matrix': confusion_matrix(y_test, y_pred)\n",
    "}\n",
    "\n",
    "print(f\"\\nüìä Results:\")\n",
    "print(f\"  Accuracy:  {results['Decision Tree']['accuracy']:.4f}\")\n",
    "print(f\"  Precision: {results['Decision Tree']['precision']:.4f}\")\n",
    "print(f\"  Recall:    {results['Decision Tree']['recall']:.4f}\")\n",
    "print(f\"  F1 Score:  {results['Decision Tree']['f1']:.4f}\")\n",
    "print(f\"  ROC AUC:   {results['Decision Tree']['roc_auc']:.4f}\")\n",
    "print(\"\\n‚úì Decision Tree trained!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Random Forest\n",
    "print(\"‚îÄ\" * 80)\n",
    "print(\"üîÑ Training: Random Forest\")\n",
    "print(\"‚îÄ\" * 80)\n",
    "\n",
    "rf_model = models['Random Forest']\n",
    "rf_model.fit(X_train, y_train)\n",
    "y_pred = rf_model.predict(X_test)\n",
    "y_pred_proba = rf_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "results['Random Forest'] = {\n",
    "    'model': rf_model,\n",
    "    'accuracy': accuracy_score(y_test, y_pred),\n",
    "    'precision': precision_score(y_test, y_pred),\n",
    "    'recall': recall_score(y_test, y_pred),\n",
    "    'f1': f1_score(y_test, y_pred),\n",
    "    'roc_auc': roc_auc_score(y_test, y_pred_proba),\n",
    "    'y_pred': y_pred,\n",
    "    'y_pred_proba': y_pred_proba,\n",
    "    'confusion_matrix': confusion_matrix(y_test, y_pred)\n",
    "}\n",
    "\n",
    "print(f\"\\nüìä Results:\")\n",
    "print(f\"  Accuracy:  {results['Random Forest']['accuracy']:.4f}\")\n",
    "print(f\"  Precision: {results['Random Forest']['precision']:.4f}\")\n",
    "print(f\"  Recall:    {results['Random Forest']['recall']:.4f}\")\n",
    "print(f\"  F1 Score:  {results['Random Forest']['f1']:.4f}\")\n",
    "print(f\"  ROC AUC:   {results['Random Forest']['roc_auc']:.4f}\")\n",
    "print(\"\\n‚úì Random Forest trained!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Gradient Boosting\n",
    "print(\"‚îÄ\" * 80)\n",
    "print(\"üîÑ Training: Gradient Boosting\")\n",
    "print(\"‚îÄ\" * 80)\n",
    "\n",
    "gb_model = models['Gradient Boosting']\n",
    "gb_model.fit(X_train, y_train)\n",
    "y_pred = gb_model.predict(X_test)\n",
    "y_pred_proba = gb_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "results['Gradient Boosting'] = {\n",
    "    'model': gb_model,\n",
    "    'accuracy': accuracy_score(y_test, y_pred),\n",
    "    'precision': precision_score(y_test, y_pred),\n",
    "    'recall': recall_score(y_test, y_pred),\n",
    "    'f1': f1_score(y_test, y_pred),\n",
    "    'roc_auc': roc_auc_score(y_test, y_pred_proba),\n",
    "    'y_pred': y_pred,\n",
    "    'y_pred_proba': y_pred_proba,\n",
    "    'confusion_matrix': confusion_matrix(y_test, y_pred)\n",
    "}\n",
    "\n",
    "print(f\"\\nüìä Results:\")\n",
    "print(f\"  Accuracy:  {results['Gradient Boosting']['accuracy']:.4f}\")\n",
    "print(f\"  Precision: {results['Gradient Boosting']['precision']:.4f}\")\n",
    "print(f\"  Recall:    {results['Gradient Boosting']['recall']:.4f}\")\n",
    "print(f\"  F1 Score:  {results['Gradient Boosting']['f1']:.4f}\")\n",
    "print(f\"  ROC AUC:   {results['Gradient Boosting']['roc_auc']:.4f}\")\n",
    "print(\"\\n‚úì Gradient Boosting trained!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üèÜ Step 7: Model Comparison\n",
    "\n",
    "Compare all models and select the best one based on F1 Score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"üèÜ MODEL COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create comparison dataframe\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': list(results.keys()),\n",
    "    'Accuracy': [r['accuracy'] for r in results.values()],\n",
    "    'Precision': [r['precision'] for r in results.values()],\n",
    "    'Recall': [r['recall'] for r in results.values()],\n",
    "    'F1 Score': [r['f1'] for r in results.values()],\n",
    "    'ROC AUC': [r['roc_auc'] for r in results.values()]\n",
    "})\n",
    "\n",
    "comparison_df = comparison_df.sort_values('F1 Score', ascending=False)\n",
    "\n",
    "print(\"\\nüìä Model Performance Comparison:\")\n",
    "display(comparison_df)\n",
    "\n",
    "# Select best model\n",
    "best_model_name = comparison_df.iloc[0]['Model']\n",
    "best_model = results[best_model_name]['model']\n",
    "\n",
    "print(f\"\\nü•á Best Model: {best_model_name}\")\n",
    "print(f\"   F1 Score: {comparison_df.iloc[0]['F1 Score']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC AUC']\n",
    "model_names = list(results.keys())\n",
    "x = np.arange(len(model_names))\n",
    "width = 0.15\n",
    "\n",
    "colors = ['#FF6B6B', '#4ECDC4', '#FFE66D', '#95E1D3', '#F38181']\n",
    "\n",
    "for idx, (metric, color) in enumerate(zip(metrics, colors)):\n",
    "    metric_key = 'f1' if metric == 'F1 Score' else metric.lower().replace(' ', '_')\n",
    "    values = [results[name][metric_key] for name in model_names]\n",
    "    ax.bar(x + idx * width, values, width, label=metric, color=color, alpha=0.8, edgecolor='black')\n",
    "\n",
    "ax.set_xlabel('Models', fontweight='bold', fontsize=12)\n",
    "ax.set_ylabel('Score', fontweight='bold', fontsize=12)\n",
    "ax.set_title('Model Performance Comparison', fontweight='bold', fontsize=16, pad=20)\n",
    "ax.set_xticks(x + width * 2)\n",
    "ax.set_xticklabels(model_names, rotation=15, ha='right')\n",
    "ax.legend(loc='lower right')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "ax.set_ylim([0, 1.1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Model comparison chart created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curves\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "colors_roc = ['#FF6B6B', '#4ECDC4', '#FFE66D', '#95E1D3']\n",
    "\n",
    "for name, color in zip(model_names, colors_roc):\n",
    "    fpr, tpr, _ = roc_curve(y_test, results[name]['y_pred_proba'])\n",
    "    auc = results[name]['roc_auc']\n",
    "    ax.plot(fpr, tpr, label=f'{name} (AUC = {auc:.3f})', linewidth=2, color=color)\n",
    "\n",
    "ax.plot([0, 1], [0, 1], 'k--', linewidth=2, label='Random Classifier')\n",
    "ax.set_xlabel('False Positive Rate', fontweight='bold', fontsize=12)\n",
    "ax.set_ylabel('True Positive Rate', fontweight='bold', fontsize=12)\n",
    "ax.set_title('ROC Curves Comparison', fontweight='bold', fontsize=16, pad=20)\n",
    "ax.legend(loc='lower right')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì ROC curves plotted!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrices\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, name in enumerate(model_names):\n",
    "    cm = results[name]['confusion_matrix']\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx], \n",
    "               cbar_kws={'label': 'Count'}, square=True, linewidths=1)\n",
    "    axes[idx].set_title(f'{name}\\nAccuracy: {results[name][\"accuracy\"]:.4f}', \n",
    "                       fontweight='bold', fontsize=12)\n",
    "    axes[idx].set_xlabel('Predicted', fontweight='bold')\n",
    "    axes[idx].set_ylabel('Actual', fontweight='bold')\n",
    "    axes[idx].set_xticklabels(['OFF (0)', 'ON (1)'])\n",
    "    axes[idx].set_yticklabels(['OFF (0)', 'ON (1)'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Confusion matrices plotted!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚öôÔ∏è Step 8: Hyperparameter Tuning (Optional)\n",
    "\n",
    "Fine-tune the best model using GridSearchCV.\n",
    "\n",
    "**Note:** This step can take several minutes. Skip if you're satisfied with current results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning for Random Forest (if it's the best model)\n",
    "if best_model_name == 'Random Forest':\n",
    "    print(\"=\" * 80)\n",
    "    print(\"‚öôÔ∏è  HYPERPARAMETER TUNING\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    param_grid = {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [10, 15, 20, None],\n",
    "        'min_samples_split': [2, 5],\n",
    "        'min_samples_leaf': [1, 2]\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nüîç Tuning {best_model_name}...\")\n",
    "    print(\"   This may take a few minutes...\\n\")\n",
    "    \n",
    "    grid_search = GridSearchCV(\n",
    "        RandomForestClassifier(random_state=42),\n",
    "        param_grid,\n",
    "        cv=5,\n",
    "        scoring='f1',\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    print(f\"\\n‚úì Hyperparameter tuning complete!\")\n",
    "    print(f\"\\nüèÜ Best Parameters:\")\n",
    "    for param, value in grid_search.best_params_.items():\n",
    "        print(f\"   {param}: {value}\")\n",
    "    \n",
    "    print(f\"\\nüìä Best Cross-Validation F1 Score: {grid_search.best_score_:.4f}\")\n",
    "    \n",
    "    # Evaluate tuned model\n",
    "    y_pred_tuned = grid_search.predict(X_test)\n",
    "    \n",
    "    print(f\"\\nüìä Tuned Model Performance on Test Set:\")\n",
    "    print(f\"   Accuracy:  {accuracy_score(y_test, y_pred_tuned):.4f}\")\n",
    "    print(f\"   Precision: {precision_score(y_test, y_pred_tuned):.4f}\")\n",
    "    print(f\"   Recall:    {recall_score(y_test, y_pred_tuned):.4f}\")\n",
    "    print(f\"   F1 Score:  {f1_score(y_test, y_pred_tuned):.4f}\")\n",
    "    \n",
    "    # Update best model\n",
    "    best_model = grid_search.best_estimator_\n",
    "    print(\"\\n‚úì Best model updated with tuned hyperparameters!\")\n",
    "else:\n",
    "    print(f\"Skipping hyperparameter tuning (best model is {best_model_name})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üì¶ Step 9: Model Quantization & Export\n",
    "\n",
    "Prepare the model for embedded deployment:\n",
    "1. Generate C code for microcontrollers\n",
    "2. Create lookup table for fast inference\n",
    "3. Export model metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"üì¶ MODEL QUANTIZATION & EXPORT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Save the best model\n",
    "joblib.dump(best_model, 'best_model.pkl')\n",
    "print(f\"\\n‚úì Saved best model: best_model.pkl\")\n",
    "print(f\"  Model: {best_model_name}\")\n",
    "\n",
    "# Save model metadata\n",
    "metadata = {\n",
    "    'model_name': best_model_name,\n",
    "    'features': ['Temp', 'Humidity', 'LDR'],\n",
    "    'target': 'Heater',\n",
    "    'performance': {\n",
    "        'accuracy': float(results[best_model_name]['accuracy']),\n",
    "        'precision': float(results[best_model_name]['precision']),\n",
    "        'recall': float(results[best_model_name]['recall']),\n",
    "        'f1_score': float(results[best_model_name]['f1']),\n",
    "        'roc_auc': float(results[best_model_name]['roc_auc'])\n",
    "    },\n",
    "    'data_info': {\n",
    "        'total_samples': len(df),\n",
    "        'training_samples': len(X_train),\n",
    "        'test_samples': len(X_test)\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('model_metadata.json', 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(f\"‚úì Saved metadata: model_metadata.json\")\n",
    "\n",
    "print(\"\\n‚úì Model export complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lookup table for embedded systems\n",
    "print(\"\\nüìä Creating quantized lookup table...\")\n",
    "\n",
    "# Create a grid of test points\n",
    "temp_range = np.arange(18, 38, 2)\n",
    "humidity_range = np.arange(70, 100, 5)\n",
    "ldr_range = np.arange(0, 101, 10)\n",
    "\n",
    "lookup_table = []\n",
    "\n",
    "for temp in temp_range:\n",
    "    for humidity in humidity_range:\n",
    "        for ldr in ldr_range:\n",
    "            features = np.array([[temp, humidity, ldr]])\n",
    "            prediction = best_model.predict(features)[0]\n",
    "            lookup_table.append({\n",
    "                'temp': int(temp),\n",
    "                'humidity': int(humidity),\n",
    "                'ldr': int(ldr),\n",
    "                'heater': int(prediction)\n",
    "            })\n",
    "\n",
    "# Save lookup table\n",
    "with open('lookup_table.json', 'w') as f:\n",
    "    json.dump(lookup_table, f, indent=2)\n",
    "\n",
    "print(f\"  ‚úì Created lookup table with {len(lookup_table)} entries\")\n",
    "print(\"  ‚úì Saved: lookup_table.json\")\n",
    "print(\"\\n‚úì Quantization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üß™ Step 10: Test Predictions\n",
    "\n",
    "Test the model with some example scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"üß™ TESTING PREDICTIONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Test cases\n",
    "test_cases = [\n",
    "    {'temp': 20, 'humidity': 75, 'ldr': 50, 'scenario': 'Cool, Low Humidity, Medium Light'},\n",
    "    {'temp': 30, 'humidity': 90, 'ldr': 80, 'scenario': 'Warm, High Humidity, Bright'},\n",
    "    {'temp': 25, 'humidity': 80, 'ldr': 30, 'scenario': 'Moderate Temp, Medium Humidity, Dim'},\n",
    "    {'temp': 18, 'humidity': 70, 'ldr': 10, 'scenario': 'Cold, Low Humidity, Dark'},\n",
    "    {'temp': 35, 'humidity': 95, 'ldr': 90, 'scenario': 'Hot, Very High Humidity, Very Bright'}\n",
    "]\n",
    "\n",
    "print(\"\\nüìã Test Predictions:\\n\")\n",
    "\n",
    "for i, case in enumerate(test_cases, 1):\n",
    "    features = np.array([[case['temp'], case['humidity'], case['ldr']]])\n",
    "    prediction = best_model.predict(features)[0]\n",
    "    proba = best_model.predict_proba(features)[0]\n",
    "    confidence = proba[prediction] * 100\n",
    "    \n",
    "    print(f\"Test Case {i}: {case['scenario']}\")\n",
    "    print(f\"  Input:  Temp={case['temp']}¬∞C, Humidity={case['humidity']}%, LDR={case['ldr']}%\")\n",
    "    print(f\"  Output: Heater {'ON' if prediction == 1 else 'OFF'} (Confidence: {confidence:.1f}%)\")\n",
    "    print()\n",
    "\n",
    "print(\"‚úì All test cases completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìä Step 11: Final Summary\n",
    "\n",
    "Generate a comprehensive summary of the ML pipeline results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"üìä FINAL SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "summary = f\"\"\"\n",
    "üéØ SMART POULTRY HEATER CONTROL SYSTEM - ML PIPELINE RESULTS\n",
    "\n",
    "üìä DATASET INFORMATION\n",
    "{'‚îÄ' * 80}\n",
    "Total Samples:     {len(df):,}\n",
    "Training Samples:  {len(X_train):,} ({len(X_train)/len(df)*100:.1f}%)\n",
    "Test Samples:      {len(X_test):,} ({len(X_test)/len(df)*100:.1f}%)\n",
    "\n",
    "Features:          Temp, Humidity, LDR\n",
    "Target:            Heater (0=OFF, 1=ON)\n",
    "\n",
    "Class Distribution:\n",
    "  OFF (0): {df['Heater'].value_counts()[0]:,} ({df['Heater'].value_counts()[0]/len(df)*100:.2f}%)\n",
    "  ON  (1): {df['Heater'].value_counts()[1]:,} ({df['Heater'].value_counts()[1]/len(df)*100:.2f}%)\n",
    "\n",
    "{'‚îÄ' * 80}\n",
    "ü§ñ MODEL PERFORMANCE\n",
    "{'‚îÄ' * 80}\n",
    "Best Model: {best_model_name}\n",
    "\n",
    "Performance Metrics:\n",
    "  Accuracy:  {results[best_model_name]['accuracy']:.4f}\n",
    "  Precision: {results[best_model_name]['precision']:.4f}\n",
    "  Recall:    {results[best_model_name]['recall']:.4f}\n",
    "  F1 Score:  {results[best_model_name]['f1']:.4f}\n",
    "  ROC AUC:   {results[best_model_name]['roc_auc']:.4f}\n",
    "\n",
    "Confusion Matrix:\n",
    "{results[best_model_name]['confusion_matrix']}\n",
    "\n",
    "{'‚îÄ' * 80}\n",
    "üì¶ GENERATED FILES\n",
    "{'‚îÄ' * 80}\n",
    "‚úì best_model.pkl              - Trained model (Python)\n",
    "‚úì model_metadata.json         - Model information\n",
    "‚úì lookup_table.json           - Prediction lookup table\n",
    "\n",
    "{'‚îÄ' * 80}\n",
    "üí° RECOMMENDATIONS\n",
    "{'‚îÄ' * 80}\n",
    "1. Deploy the model using the lookup table for fastest inference\n",
    "2. Monitor prediction confidence and flag low-confidence cases\n",
    "3. Consider retraining if sensor ranges extend beyond current data\n",
    "4. Implement data logging for continuous model improvement\n",
    "\n",
    "{'=' * 80}\n",
    "\"\"\"\n",
    "\n",
    "print(summary)\n",
    "\n",
    "# Save summary to file\n",
    "with open('ML_PIPELINE_SUMMARY.txt', 'w') as f:\n",
    "    f.write(summary)\n",
    "\n",
    "print(\"‚úì Summary saved to: ML_PIPELINE_SUMMARY.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéâ Pipeline Complete!\n",
    "\n",
    "### ‚úÖ What We Accomplished:\n",
    "\n",
    "1. ‚úÖ **Data Exploration** - Analyzed 60,000 samples\n",
    "2. ‚úÖ **Visualizations** - Created comprehensive charts\n",
    "3. ‚úÖ **Model Training** - Trained 4 different models\n",
    "4. ‚úÖ **Model Comparison** - Identified best performer\n",
    "5. ‚úÖ **Hyperparameter Tuning** - Optimized the model\n",
    "6. ‚úÖ **Model Export** - Saved deployment artifacts\n",
    "7. ‚úÖ **Quantization** - Created lookup table\n",
    "\n",
    "### üìÅ Generated Files:\n",
    "\n",
    "- `best_model.pkl` - Trained model\n",
    "- `model_metadata.json` - Model information\n",
    "- `lookup_table.json` - Prediction table\n",
    "- `ML_PIPELINE_SUMMARY.txt` - Results summary\n",
    "\n",
    "### üöÄ Next Steps:\n",
    "\n",
    "1. **Download the model files** from Colab\n",
    "2. **Deploy to microcontroller** using the lookup table\n",
    "3. **Integrate with web interface** for monitoring\n",
    "4. **Test with real sensors** in the field\n",
    "\n",
    "---\n",
    "\n",
    "**üêî Your Smart Poultry Heater Control System is ready for deployment! üåæ**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üíæ Download Files from Colab\n",
    "\n",
    "Run this cell to download all generated files to your local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download files (uncomment if running in Colab)\n",
    "# from google.colab import files\n",
    "\n",
    "# files.download('best_model.pkl')\n",
    "# files.download('model_metadata.json')\n",
    "# files.download('lookup_table.json')\n",
    "# files.download('ML_PIPELINE_SUMMARY.txt')\n",
    "\n",
    "print(\"‚úì Ready to download files!\")\n",
    "print(\"\\nUncomment the code above to download files from Colab.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
